{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='hzz.png' width=\"570\"></td><td><img src='ATLAS-figaux_01.png' width=\"800\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How much Higgs? - Intro\n",
    "- **Overview**\n",
    "    - Welcome to the tutorial that covers the first half of the Data Science in High Energy Physics module. The notebook is based on statistical analysis of open data and simulation from the ATLAS experiment at CERN.  Even if the physics context of this data is not entirely clear to you, it should not be a problem as understanding anything beyond the next paragraph is not neccessary.\n",
    "\n",
    "- **The physics in a nutshell**\n",
    "    - The Higgs boson was theorised in the 1960s as the fundamental particle that is responsible for giving mass to other fundamental particles. The problem is, the Higgs boson is extremely difficult to produce in lab conditions. Hence the Large Hadron Collider had to be built at CERN in order to provide enough high energy particle collisions to produce a significant Higgs bosons such that we could confirm the existence of this elusive boson. In 2012, the ATLAS and CMS experiments at the LHC announced that they had finally discovered the Higgs boson and the Nobel Prize in physics was subsequently awarded to Peter Higgs and Francois Englert for the boson's prediction. One of the easiest ways to find a Higgs boson in the ATLAS data is to look for collisions (often called 'events') containing 4 *leptons* which can be either electrons or muons. Muons are the slightly more massive cousin of the electron. This is because the Higgs boson can decay into two Z bosons, which in turn each decay into two leptons, leading to the four lepton signal. This process is depicted in the left hand diagram above where (reading left to right) two gluons collide to form a Higgs boson, which subsequently decays to two Z bosons and then four leptons. The right had image above shows a computerised illustration of **real** collision from the ATLAS data that contained four muons. In this notebook we will search through the ATLAS data for these events and compare what we find to predictions. \n",
    "\n",
    "- **Technical details**\n",
    "    - The notebook contains code that performs a simple analysis of the data and simulation. All code is Python 3 and utilised quite a few python package which must be installed before running the notebook. All required packages are listed in the 'requirements.txt' file.  The code is interspersed with exercises where you are either asked to discuss the analysis in the previous cell and/or extend the existing analysis. Specifically, we will look at collisions contains 4 leptons (electrons or muons) and use a histogram of the masses of the 4 lepton system ($m_{llll}$) to play with some statistical ideas. In the last few excercises you are required to perform some analyses with less guidance.\n",
    "    - This notebook has been tested with python 3.10. I recommend setting up a python virtual environment or simailr and NOT using your system python (this will quickly become impossible to manage). \n",
    "\n",
    "- **Report**\n",
    "    - You need to submit a 3-4 page report on this tutorial containg:\n",
    "        - an introduction explaining, in your own words, the overall concept of the tutorial and the exercises therein. \n",
    "        - your solution for each exercise (including any plots you make or need to explain your answer)\n",
    "            - code you developed to perform the last two exercises may be includes as an appendix to the 3-4 page report\n",
    "        - a brief conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some crucial python modules that will allow us to analyse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot3 as uproot\n",
    "import uproot3_methods.classes.TLorentzVector as LVepm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import infofile\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following helper function to help use to calibrate the simulation, we dont need to worry about what it actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample):\n",
    "    info = infofile.infos[sample] # open infofile\n",
    "    xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "    return xsec_weight # return cross-section weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just defining the names of the files from where we will read the data and simulation. All data and MC samples should be downloaded from the Data Science folder on the Physics Honours Amathuba site.\n",
    "The last file is the real collision data from ATLAS, the others are simulations of the physics processes we expect to have aoccured in the collisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "\"mc_361106.Zee.4lep.root\",\n",
    "\"mc_361107.Zmumu.4lep.root\",\n",
    "\"mc_410000.ttbar_lep.4lep.root\",\n",
    "\"mc_363490.llll.4lep.root\",\n",
    "\"mc_363492.llvv.4lep.root\",\n",
    "\"mc_363356.ZqqZll.4lep.root\",    \n",
    "\"mc_345060.ggH125_ZZ4lep.4lep.root\",\n",
    "\"mc_341964.WH125_ZZ4lep.4lep.root\",\n",
    "\"mc_344235.VBFH125_ZZ4lep.4lep.root\",\n",
    "\"mc_341947.ZH125_ZZ4lep.4lep.root\",\n",
    "\"data.4lep.root\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select out the most interesting collisions from the data and simulation so that we can as clear as possible signal of Higgs boson production. The exact details of the cuts are not very important. In summary, we select collision in which four leptons (electrons or muons) have been detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mc_361106.Zee.4lep.root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m sample_name \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \n\u001b[1;32m     24\u001b[0m sample_names\u001b[38;5;241m.\u001b[39mappend(sample_name)\n\u001b[0;32m---> 25\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43muproot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m mcWeight, SumWeights, XSection, trigM, trigE, scaleFactor_PILEUP, scaleFactor_ELE, scaleFactor_MUON,scaleFactor_LepTRIGGER, lep_type, lep_pt, lep_eta, lep_phi, lep_E, lep_charge, lep_etcone20, lep_ptcone30, jet_n, jet_pt, jet_eta, jet_phi, jet_E, jet_MV2c10 \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39marrays([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmcWeight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSumWeights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXSection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaleFactor_PILEUP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaleFactor_ELE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaleFactor_MUON\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaleFactor_LepTRIGGER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_eta\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_phi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_E\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_charge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_etcone20\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlep_ptcone30\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_n\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_eta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_phi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_E\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_MV2c10\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile has been successfully opened!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Physics/DataScience/Mini_Project/Data_Science_in_High_Energy_Physics/dataScience_env/lib/python3.10/site-packages/uproot3/rootio.py:55\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, localsource, xrootdsource, httpsource, **options)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m         openfcn \u001b[38;5;241m=\u001b[39m localsource\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ROOTDirectory\u001b[38;5;241m.\u001b[39mread(\u001b[43mopenfcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _bytesid(parsed\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xrootd(path, xrootdsource\u001b[38;5;241m=\u001b[39mxrootdsource, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Documents/Physics/DataScience/Mini_Project/Data_Science_in_High_Energy_Physics/dataScience_env/lib/python3.10/site-packages/uproot3/rootio.py:52\u001b[0m, in \u001b[0;36mopen.<locals>.<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m options:\n\u001b[1;32m     51\u001b[0m             kwargs[n] \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(n)\n\u001b[0;32m---> 52\u001b[0m     openfcn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m path: \u001b[43mMemmapSource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     openfcn \u001b[38;5;241m=\u001b[39m localsource\n",
      "File \u001b[0;32m~/Documents/Physics/DataScience/Mini_Project/Data_Science_in_High_Energy_Physics/dataScience_env/lib/python3.10/site-packages/uproot3/source/memmap.py:21\u001b[0m, in \u001b[0;36mMemmapSource.__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(path)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_source \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Physics/DataScience/Mini_Project/Data_Science_in_High_Energy_Physics/dataScience_env/lib/python3.10/site-packages/numpy/core/memmap.py:228\u001b[0m, in \u001b[0;36mmemmap.__new__\u001b[0;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[1;32m    226\u001b[0m     f_ctx \u001b[38;5;241m=\u001b[39m nullcontext(filename)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     f_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m f_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    231\u001b[0m     fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mc_361106.Zee.4lep.root'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#------------DEFINING SOME VARIABLES AND OBJECTS FOR THE ANALYSIS----------------------------------------------------\n",
    "\n",
    "lumi = 10#fb^-1\n",
    "nBins = 34\n",
    "\n",
    "minMass = 80\n",
    "maxMass = 250\n",
    "\n",
    "bins_ar = np.linspace(minMass, maxMass, num=(nBins+1))\n",
    "mc_hist_list = []\n",
    "sample_names = []\n",
    "\n",
    "f = plt.figure()\n",
    "\n",
    "H_125 = np.zeros([nBins])\n",
    "H_bkg = np.zeros([nBins])\n",
    "\n",
    "btagWP77 = 0.6459\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for file in files: #looping over data and simulation files\n",
    "    sample_name = file.split(\".\")[1] \n",
    "    sample_names.append(sample_name)\n",
    "    tree = uproot.open(file)[\"mini\"]\n",
    "\n",
    "    mcWeight, SumWeights, XSection, trigM, trigE, scaleFactor_PILEUP, scaleFactor_ELE, scaleFactor_MUON,scaleFactor_LepTRIGGER, lep_type, lep_pt, lep_eta, lep_phi, lep_E, lep_charge, lep_etcone20, lep_ptcone30, jet_n, jet_pt, jet_eta, jet_phi, jet_E, jet_MV2c10 = tree.arrays([\"mcWeight\", \"SumWeights\", \"XSection\",\"trigM\", \"trigE\",\"scaleFactor_PILEUP\", \"scaleFactor_ELE\", \"scaleFactor_MUON\",\"scaleFactor_LepTRIGGER\",\"lep_type\",\"lep_pt\", \"lep_eta\",\"lep_phi\", \"lep_E\", \"lep_charge\", \"lep_etcone20\", \"lep_ptcone30\", \"jet_n\", \"jet_pt\", \"jet_eta\", \"jet_phi\",\"jet_E\", \"jet_MV2c10\"], outputtype=tuple)\n",
    "    print(\"File has been successfully opened!\")\n",
    "    print (type(lep_pt))\n",
    "    \n",
    "    leplv = LVepm.TLorentzVectorArray.from_ptetaphi(lep_pt, lep_eta, lep_phi, lep_E)\n",
    "\n",
    "    lep_reliso_pt = (lep_ptcone30 / lep_pt)\n",
    "    lep_reliso_et = (lep_etcone20 / lep_pt)\n",
    "    sum_lep_type = lep_type.sum()\n",
    "    \n",
    "    jetlv = LVepm.TLorentzVectorArray.from_ptetaphi(jet_pt, jet_eta, jet_phi, jet_E)\n",
    "    jetlv = jetlv[jet_MV2c10.argsort()]    \n",
    "    tags = jet_pt[jet_MV2c10 > btagWP77]\n",
    "\n",
    "    trig_cut = ( (trigM==1) | (trigE==1))\n",
    "    lep_kinematics_cut  = ( (lep_pt.max() > 20000) & (lep_pt.min() > 7000) & (lep_eta.min() >-2.5) & (lep_eta.max() < 2.5))\n",
    "    lep_type_cut  = ((sum_lep_type == 44) | (sum_lep_type == 48) | (sum_lep_type == 52))\n",
    "    lep_iso_cut =  ((lep_reliso_pt.max() < 0.3) & (lep_reliso_pt.max() < 0.3))\n",
    "    lept_count_cut = (leplv.counts ==4)\n",
    "    lept_charge_cut = (lep_charge.sum()==0)\n",
    "    \n",
    "    # filtering events according to cuts above that select out interesting events\n",
    "    event_cut = ( lep_kinematics_cut & lep_type_cut & lep_iso_cut  & lept_count_cut & lept_charge_cut)\n",
    "\n",
    "    first_lep_p4 =  leplv[event_cut,0]\n",
    "    second_lep_p4 = leplv[event_cut,1]\n",
    "    third_lep_p4 =  leplv[event_cut,2]\n",
    "    fourth_lep_p4 = leplv[event_cut,3]\n",
    "    mcWeight = mcWeight[event_cut] \n",
    "    \n",
    "    scaleFactor_PILEUP = scaleFactor_PILEUP[event_cut] \n",
    "    scaleFactor_ELE = scaleFactor_ELE[event_cut] \n",
    "    scaleFactor_MUON = scaleFactor_MUON[event_cut] \n",
    "    scaleFactor_LepTRIGGER = scaleFactor_LepTRIGGER[event_cut] \n",
    "    \n",
    "    #construct 4-lepton system by adding 4-vectors of the leptons vectorially\n",
    "    llll_p4 = first_lep_p4 + second_lep_p4 + third_lep_p4 + fourth_lep_p4  \n",
    "    \n",
    "    \n",
    "    # make histograms of the m_llll distribution for simulation and data\n",
    "    if(file.split(\"_\")[0] == \"mc\"):\n",
    "        finalWeight = get_xsec_weight(sample_name)*(mcWeight)*(scaleFactor_PILEUP)*(scaleFactor_ELE) *(scaleFactor_MUON)*(scaleFactor_LepTRIGGER)\n",
    "        H, b = np.histogram(llll_p4.mass/1000.0, weights=finalWeight, bins=bins_ar)                        \n",
    "        mc_hist_list.append(H)\n",
    "        if(\"H125\" in file):\n",
    "            H_125 = np.add(H, H_125)\n",
    "        else:\n",
    "            #print(\"Sample Name = \" + str(file) + \" exp. num. events = \" + str(np.sum(finalWeight)) )\n",
    "            H_bkg = np.add(H, H_bkg)\n",
    "    else:\n",
    "        finalWeight = np.ones(len(mcWeight)) \n",
    "        sample_name = \"Data (10 fb^-1)\"\n",
    "        H_data, b = np.histogram(llll_p4.mass/1000.0, weights=finalWeight, bins=bins_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our events selected, we can compare our data with our expectations from theory. Our best theory of particle physics is known as the Standard Model, so the predictions here represent the expectations from the Standard Model after a simulation of how the ATLAS detector detects particles has been applied. \n",
    "\n",
    "Specifically, we plot a histogram of the number of collisions (\"events\") as a function of the mass of the four-lepton system. We use this mass because it distinguishes between the the Higgs signal, which has a peak at the higgs mass (125 GeV) and is zero elesehere, and the backgrounds which have a peak around 90 GeV and are flatter elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "hep.histplot([H_bkg, H_125], bins=bins_ar, stack=True, label=[\"Backgrounds\", \"Higgs\"], histtype='fill')\n",
    "hep.histplot([H_data], bins=bins_ar, stack=False, yerr=True, histtype=\"errorbar\", color=\"black\",label=\"ATLAS Open Data\")\n",
    "\n",
    "plt.legend(loc=1, ncol=3, fontsize=9)\n",
    "plt.xlabel(\"m_llll [GeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.ylim([0.0, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 \n",
    " Discuss if the model(coloured histograms) by the is a good model for the data (black points) and why.\n",
    " Calculation of statistics to make your answer more quantitative is encouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first must define a chi-squared function that will quantify how closely the model and data agree. It should take the observed bin heights of the data and  prediect bin heights of the model as inputs. The data can be assumed to be poisson-distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def calcChiSq(obs, preds):\n",
    "    #REPLACE THIS FUNCTION WITH A VALID CHI-SQUARED CALCULATION\n",
    "    chiSq=1\n",
    "    ndf=1\n",
    "    return chiSq, ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting s_s\n",
    "  We can make the crude assumption that the only thing wrong with our model is that the predicted total number of events from the Higgs signal is wrong by some single multiplicative factor which we will call s_s. We can investigate then how the agreement between the model and the data could be improve by applying various values of s_s and observing the change in the chi-squared. \n",
    "  \n",
    "  This code will not give any sensibel results without a valid chi-squared calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "s_s_ar = np.linspace(1.0, 3.0, 100) # array of s_s values we will investigate\n",
    "\n",
    "chi2_ar = np.empty( len(s_s_ar) ) # empty array of that will hold the chi2 values we will calculate\n",
    "\n",
    "minChi2 = 1000000\n",
    "bestFit_s_s = 0.0 #starting values for min chi2 and best value of s_s\n",
    "\n",
    "for s_s in range(0, len(s_s_ar)): #looping over out s_s values \n",
    "    pred = (s_s_ar[s_s]*H_125) + (H_bkg) # generating a prediction according to this s_s\n",
    "    chi2 = calcChiSq(H_data, pred)[0] # calculating chi2\n",
    "    chi2_ar[s_s] = chi2 # adding chi2 value to chi2 array \n",
    "    if(chi2 < minChi2): # check if this is the lowest chi2 we have seen so far\n",
    "        minChi2 = chi2 # update lowest chi2 value seen \n",
    "        bestFit_s_s = s_s_ar[s_s] # update value for best fit s_s\n",
    "        \n",
    "deltaChi2_ar = chi2_ar - minChi2 # make array of delta chi2 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have implmented a correct chi-squared calcualtion in the earlier cell. You should now have an array of delta chi2 values. The next cell will give an \"out of bounds error\" if you have not yet implmented your proper chi-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#we expect the chi2 vs. mZ curve to be quadratic, so let's fit that function to it.\n",
    "z = np.polyfit(s_s_ar, deltaChi2_ar, 2) #\"2\" for a second-order polynomial\n",
    "\n",
    "\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# we can display the estimated uncertianty on mZ via critical values of the delta chi-squared curve\n",
    "y0 = 1.0\n",
    "crit = (p - y0).roots # roots of the polynominal -1, i.e., the mz values where p = 1 \n",
    "\n",
    "#shading in the confience interval band \n",
    "print(crit)\n",
    "px=np.arange(crit[1],crit[0],0.001)\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_s_ar, deltaChi2_ar, 'k', linewidth=2)\n",
    "ax.fill_between(px,p(px),alpha=0.5, color='g', label=\"uncertainty\")\n",
    "\n",
    "plt.xlabel(\"s_s\")\n",
    "plt.ylabel(\"delta chi-squared\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Best fit value of s_s = \" + str(round(bestFit_s_s, 3)) + \" +/- \" + str(round(np.abs(crit[0] - bestFit_s_s),2 )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "  Discuss the results of the fitting of s_s. Is the fitted model now a successful mode for the data? How can you quantify this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting s_b\n",
    "  Alternatively, we can make the crude assumption that the only thing wrong with our model is that the predicted total number of events from the *backgrounds* is wrong by some single multiplicative factor which we will call s_b.\n",
    " We can investigate then how the agreement between the model and the data could be improve by applying various values of s_s and observing the change in the chi-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "s_b_ar = np.linspace(1.15, 1.4, 100)\n",
    "\n",
    "chi2_ar = np.empty( len(s_b_ar) )\n",
    "\n",
    "minChi2 = 1000000\n",
    "bestFit_s_b = 0.0\n",
    "\n",
    "for s_b in range(0, len(s_b_ar)):\n",
    "    pred = (H_125) + (s_b_ar[s_b]*H_bkg)\n",
    "    chi2 = calcChiSq(H_data, pred)[0]\n",
    "    chi2_ar[s_b] = chi2\n",
    "    if(chi2 < minChi2):\n",
    "        minChi2 = chi2\n",
    "        bestFit_s_b = s_b_ar[s_b]\n",
    "        \n",
    "deltaChi2_ar = chi2_ar - minChi2\n",
    "\n",
    "z = np.polyfit(s_b_ar, deltaChi2_ar, 2)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "y0 = 1.0 # this is the value of the delta chi-squared function that defines the 68% CI for a one parameter fit.\n",
    "         # we'll invetigate if this parmaeter makes sense in the final exercise.\n",
    "crit = (p - y0).roots \n",
    "\n",
    "px=np.arange(crit[1],crit[0],0.001)\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_b_ar, deltaChi2_ar, 'k', linewidth=2)\n",
    "ax.fill_between(px,p(px),alpha=0.5, color='g', label=\"uncertainty\")\n",
    "plt.xlabel(\"s_b\")\n",
    "plt.ylabel(\"delta chi-squared\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Best fit value of s_b = \" + str(round(bestFit_s_b, 3)) + \" +/- \" + str(round(np.abs(crit[0] - bestFit_s_b),2 )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Discuss the results of the fitting of s_b. Is the fitted model now a successful mode for the data? Is it better than the previous s_s model? How can you quantify this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting s_s and s_b\n",
    "More realistically, we can assume that the the predicted total numbers of events from both the Higgs signal AND the backgrounds are wrong by some two seperate multiplicative factors. We can investigate then how the agreement between the model and the data could be improve by applying various values of (s_s, s_b) and observing the change in the chi-squared. We now need to plot the delta chi-squared as a 2-D function of (s_s, s_b) as it depends on both parmaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "s_s_ar = np.linspace(0.1, 2.7, 100)\n",
    "s_b_ar = np.linspace(1.1, 1.5, 100)\n",
    "\n",
    "chi2_ar = np.empty( (len(s_s_ar), len(s_b_ar) ))\n",
    "\n",
    "bestFit_s_s = 0.0\n",
    "bestFit_s_b = 0.0\n",
    "\n",
    "minChi2 = 1000000\n",
    "\n",
    "for s_s in range(0, len(s_s_ar)):\n",
    "    for s_b in range(0, len(s_b_ar)):\n",
    "        pred = (s_s_ar[s_s]*H_125) + (s_b_ar[s_b]*H_bkg)\n",
    "        chi2 = calcChiSq(H_data, pred)[0]\n",
    "        chi2_ar[s_s, s_b] = chi2 \n",
    "        if(chi2 < minChi2):\n",
    "            minChi2 = chi2\n",
    "            bestFit_s_s = s_s_ar[s_s]\n",
    "            bestFit_s_b = s_b_ar[s_b]\n",
    "\n",
    "deltaChi2_ar = chi2_ar - minChi2\n",
    "\n",
    "levels = [2.3] # this is the value of the delta chi-squared function that defines the 68% CI for a two parameter fit.\n",
    "         # we'll invetigate if this parmaeter makes sense in the final exercise.\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.gca()\n",
    "\n",
    "cfset = plt.contourf(s_b_ar, s_s_ar, deltaChi2_ar,  cmap='coolwarm')\n",
    "cset = plt.contour(s_b_ar, s_s_ar, deltaChi2_ar, levels=levels, colors=['white'])\n",
    "\n",
    "bf_point = plt.scatter(bestFit_s_b, bestFit_s_s, color='black')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(cfset)\n",
    "cbar.set_label('Delta Chi-squared', fontsize=15, rotation=270)\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('s_b',fontsize=18)\n",
    "ax.set_ylabel('s_s',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "#extract 2D result\n",
    "print(\"best-fit s_s = \" + str(bestFit_s_s))\n",
    "print(\"best-fit s_b = \" + str(bestFit_s_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Discuss the results of the simulatenous fit of s_s and s_b. How do the results of the three fitting approaches compare? If we interpret the Confidence Intervals on s_s and s_b as uncertainties on measurements of these parameters, then how do the uncertainties compare? Have we measured one parameter better than the other? If so, why? Does it make sense given the histograms we used to perform the fit of these parameters? How do the best-fit vales of the s_s and s_b comapre to the values when we fit one parameter at a time? Discuss what can be learned from the shape of the contours of the 2-D delta-chi-squared function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a 3D version of the 2-D contour Confidence Interval plot we made earlier, for no reason other than it looks cool and making cool plots is one of the main reasons data analysis is fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(s_b_ar, s_s_ar, deltaChi2_ar, 80, cmap='coolwarm')\n",
    "ax.set_xlabel('s_b')\n",
    "ax.set_ylabel('s_s')\n",
    "ax.set_zlabel('delta chi-squared')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have estimates of the 'best-fit' values of s_s and s_b. Let's apply these factors to the predictions for signal and background and see how the updated predctions compare to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "\n",
    "H_125_bf = bestFit_s_s*H_125\n",
    "H_bkg_bf = bestFit_s_b*H_bkg\n",
    "\n",
    "hep.histplot([H_bkg_bf, H_125_bf], bins=bins_ar, stack=True, label=[\"Backgrounds (best-fit)\", \"Higgs (best-fit)\"], histtype='fill')\n",
    "hep.histplot([H_data], bins=bins_ar, stack=False, yerr=True, histtype=\"errorbar\", color=\"black\",label=\"ATLAS Open Data\")\n",
    "plt.legend(loc=1, ncol=2, fontsize=9)\n",
    "plt.xlabel(\"m_llll [GeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.ylim([0.0, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data and our best-fit predictions agree very well... right?.......right??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodness of fit\n",
    "We see that our fit succeeded it produced predictions for our signal and background that seem to agree much more closely with data than the original predictions. However, are we sure that our fitted model is a 'good' model for the data? We could as the question: \"If this is the correct model, then how probable is our observed data?\". This is answered by working our the p-value of our observed data acording to our fitted model. \n",
    "\n",
    "In the cell below, we take out fitted model and assume each bin is distrubuted as a gaussian with mean equal to the prediction in the bin and standard deviation equal to the square root of the mean. We generate N toy from the this fitted model and calculate the chi-squared for each toy with respect to the fitted model.\n",
    "\n",
    "Overlaying the chi-squared value of our ATLAS data with respect to our model \"If this is the correct model, then how probable is our observed data?\". This is answered by working our the p-value of our observed data acording to our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# generate the chi-2 distribution of N toy experiments based on the fitted model \n",
    "# assume a gaussian-distributed bin height with mean = model pred. and sigma equal root model pred\n",
    "# what then is the p-value of the data with respect to the distribution?\n",
    "    \n",
    "#function to generate a toy histogram given the mean for each bin and assuming the data is\n",
    "# gaussian-distrubtu\n",
    "def generate_toy(means): \n",
    "        toy = np.empty(len(means))\n",
    "        for i in range(0, len(means)):\n",
    "            toy_bin = np.random.normal(means[i], np.sqrt(means[i]), 1)\n",
    "            toy[i] = toy_bin\n",
    "        #print(\"toy = \" + str(toy))\n",
    "        return toy\n",
    "\n",
    "ntoys = 10000 # start with a small number, increase when you undertsand your results\n",
    "\n",
    "means = H_bkg_bf + H_125_bf # take the results of the fit as the means of the fitted model\n",
    "\n",
    "#print(\"means\" + str(means))\n",
    "\n",
    "chi2_toys = np.empty(ntoys)\n",
    "\n",
    "for t in range(0, ntoys):\n",
    "    toy = generate_toy(means) \n",
    "    chi2_toys[t] = calcChiSq(toy, means)[0]\n",
    "    #print(\"chi2 \" + str(chi2_toys[t]))\n",
    "    \n",
    "plt.figure()\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "#plot distribtion of chi-squared values from toys\n",
    "bins_ar = np.linspace(0.0, 200, num=(nBins+1))\n",
    "chi2Hist, chi2bins = np.histogram(chi2_toys, bins=bins_ar, density=True)                        \n",
    "hep.histplot([chi2Hist], bins=chi2bins, histtype='fill', label=\"toys\")\n",
    "\n",
    "#plot expected distrutuon of chi-squared values from theory - chi-squared distribtuions with ndof = nbins -2\n",
    "df = nBins - 2\n",
    "x = np.linspace(stats.chi2.ppf(0.000001, df), stats.chi2.ppf(0.999999, df), 200)\n",
    "label = '$\\chi^{2}$ (ndof = ' + str(df) + ') pdf'\n",
    "ax1.plot(x, stats.chi2.pdf(x, df), 'r-', lw=5, alpha=0.6, label=label)\n",
    "\n",
    "# overlay the chi2 squared value from the ATLAS OpenData\n",
    "chi2_data = calcChiSq(H_data, means)[0]\n",
    "plt.plot(chi2_data,0.002 , 'y*', ms=14, label='ATLAS OpenData')\n",
    "plt.legend()\n",
    "plt.xlabel(\"chi^2\")\n",
    "plt.ylabel(\"# toys\")\n",
    "plt.xlim(0,100)\n",
    "\n",
    "print(\"chi2 value of ATLAS Open data is = \" + str(chi2_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Goodness of fit and p-value\n",
    "Calculate the p-value of the ATLAS data with respect to the chi-squared distribution shown above. Comment if this value indicates if the fitted model is a good model for the ATLAS data or not. Repeat the procedure to calculate the p-value of the data with respect to the model **before** any fitting occured. How do the two p-values compare? What do you conclude about the fitted model? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 6 - How do we know the critical value for a Confidence Interval?\n",
    "Earlier we used critical value of 2.3 of the delta chi-squared function to define the 68% Confidence Interval on our fit parameters. This value of 2.3 is based on the propertires of the Gaussian distribution. In this exercise we aim to check that this value of 2.3 indeed gives us the correct 68 % CL. Recall the definition of a X % CL: *an interval constructed such that if we constructed this interval in the same way for a large number of repeated experiments, it would contain the true value of the parameter in X % of those experiments*.\n",
    "\n",
    "Use random numbers to work out the critical value of the delta chi-squared corresponding to the boundaries of the 68 % Confidence Interval for the fit performed above. Some hints are given towards the steps needed in the cell below. The steps are not neccessarily in the correct order, they rather they repreent the crucial pieces of information you need to build a solution.\n",
    "\n",
    "N.B. your setup will likely not initialy give you exaclty the correct critical values with respect to the theoretical expectation? What needs to be done such that the exact critical values can be determined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# suggested steps\n",
    "\n",
    "    # generate N toys\n",
    "\n",
    "        # for each toy, run 2-parameter fit and estimate CL based on critical value d\n",
    "        # (note that you will fit the toy to the 'true' distributions, not real data)\n",
    "\n",
    "        # check if true value is within the contour of the delta chiquared function defined by d\n",
    "\n",
    "    # calculate fraction of toys (f) in which the CL estimation for critical value\n",
    "    # contains true values of s_s and s_b\n",
    "\n",
    "    # if you can check the fraction of toys with CI containing the true value \n",
    "    # or a given d, you just need to repeat this  for a range of d values.\n",
    "    # this will take some time, so start with a small number of toys and a few \n",
    "    # d values, when you understand your results you can increse both.\n",
    "\n",
    "    # plot f as a function of d\n",
    "\n",
    "    # read form this plot the critical value that gives you the 68% CI.\n",
    "    \n",
    "# Best of luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
