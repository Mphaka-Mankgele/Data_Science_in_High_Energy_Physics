{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHt4elE+GcITb+oAI7sfGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mphaka-Mankgele/Data_Science_in_High_Energy_Physics/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score, roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the transform with data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Function to create data loaders\n",
        "def create_data_loaders(batch_size):\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = nn.ReLU()(self.bn2(self.conv2(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "probabilities = []\n",
        "y_true = []\n",
        "# Function to train the model\n",
        "def train_model(train_loader, test_loader, learning_rate, batch_size, dropout_rate, num_epochs=100):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleCNN(dropout_rate=dropout_rate).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Calculate loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    y_pred = []\n",
        "    all_outputs = []  # For ROC curve\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            all_outputs.append(outputs.cpu())  # Collect outputs for ROC\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            y_true.extend(labels.cpu().numpy())  # Add true labels\n",
        "            y_pred.extend(predicted.cpu().numpy())  # Add predicted labels\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')  # Calculate F1 score\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    all_outputs = torch.cat(all_outputs).numpy()  # Combine all outputs\n",
        "    # Convert outputs to probabilities using softmax\n",
        "    probabilities = nn.functional.softmax(torch.tensor(all_outputs), dim=1).numpy()\n",
        "\n",
        "    # Calculate ROC curve for each class\n",
        "    num_classes = 10\n",
        "    roc_auc = {}\n",
        "    for i in range(num_classes):\n",
        "        fpr, tpr, _ = roc_curve(np.array(y_true) == i, probabilities[:, i])  # True positive rates\n",
        "        roc_auc[i] = auc(fpr, tpr)\n",
        "\n",
        "    return test_accuracy, f1, roc_auc\n",
        "\n",
        "# Example usage\n",
        "learning_rate = 0.0001\n",
        "batch_size = 64\n",
        "dropout_rate = 0.5\n",
        "train_loader, test_loader = create_data_loaders(batch_size)\n",
        "test_accuracy, f1, roc_auc = train_model(train_loader, test_loader, learning_rate, batch_size, dropout_rate)\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%, F1 Score: {f1:.4f}')\n",
        "for class_idx, auc_value in roc_auc.items():\n",
        "    print(f'Class {class_idx}: ROC AUC = {auc_value:.4f}')\n",
        "\n",
        "# Optional: Plot ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(10):\n",
        "    fpr, tpr, _ = roc_curve(np.array(y_true) == i, probabilities[:, i])\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfyIjj-JAmJH",
        "outputId": "ae8eebe2-f203-412c-da5b-eeee14a99ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/100], Loss: 1297.9529, Train Accuracy: 39.54%\n",
            "Epoch [2/100], Loss: 1074.3776, Train Accuracy: 50.28%\n",
            "Epoch [3/100], Loss: 982.9426, Train Accuracy: 55.05%\n",
            "Epoch [4/100], Loss: 933.1585, Train Accuracy: 57.28%\n",
            "Epoch [5/100], Loss: 894.4867, Train Accuracy: 59.22%\n",
            "Epoch [6/100], Loss: 864.3475, Train Accuracy: 60.72%\n",
            "Epoch [7/100], Loss: 839.4826, Train Accuracy: 62.23%\n",
            "Epoch [8/100], Loss: 817.3787, Train Accuracy: 63.13%\n",
            "Epoch [9/100], Loss: 797.2815, Train Accuracy: 63.94%\n",
            "Epoch [10/100], Loss: 785.3640, Train Accuracy: 64.39%\n",
            "Epoch [11/100], Loss: 763.9933, Train Accuracy: 65.42%\n",
            "Epoch [12/100], Loss: 756.0867, Train Accuracy: 66.06%\n",
            "Epoch [13/100], Loss: 741.7495, Train Accuracy: 66.65%\n",
            "Epoch [14/100], Loss: 733.0374, Train Accuracy: 67.12%\n",
            "Epoch [15/100], Loss: 722.6568, Train Accuracy: 67.58%\n",
            "Epoch [16/100], Loss: 712.2776, Train Accuracy: 67.88%\n",
            "Epoch [17/100], Loss: 705.6318, Train Accuracy: 68.22%\n",
            "Epoch [18/100], Loss: 699.0869, Train Accuracy: 68.54%\n",
            "Epoch [19/100], Loss: 689.4012, Train Accuracy: 68.84%\n",
            "Epoch [20/100], Loss: 679.9410, Train Accuracy: 69.48%\n",
            "Epoch [21/100], Loss: 676.4583, Train Accuracy: 69.68%\n",
            "Epoch [22/100], Loss: 671.0974, Train Accuracy: 70.09%\n",
            "Epoch [23/100], Loss: 664.8858, Train Accuracy: 70.28%\n",
            "Epoch [24/100], Loss: 657.7788, Train Accuracy: 70.55%\n",
            "Epoch [25/100], Loss: 653.7273, Train Accuracy: 70.45%\n",
            "Epoch [26/100], Loss: 651.0755, Train Accuracy: 70.80%\n",
            "Epoch [27/100], Loss: 644.3091, Train Accuracy: 71.18%\n",
            "Epoch [28/100], Loss: 639.6444, Train Accuracy: 71.26%\n",
            "Epoch [29/100], Loss: 634.1179, Train Accuracy: 71.48%\n",
            "Epoch [30/100], Loss: 631.1504, Train Accuracy: 71.78%\n",
            "Epoch [31/100], Loss: 626.1571, Train Accuracy: 71.82%\n",
            "Epoch [32/100], Loss: 622.8778, Train Accuracy: 72.03%\n",
            "Epoch [33/100], Loss: 618.0262, Train Accuracy: 72.27%\n",
            "Epoch [34/100], Loss: 614.5262, Train Accuracy: 72.51%\n",
            "Epoch [35/100], Loss: 609.5599, Train Accuracy: 72.79%\n",
            "Epoch [36/100], Loss: 606.4767, Train Accuracy: 72.87%\n",
            "Epoch [37/100], Loss: 605.0760, Train Accuracy: 73.00%\n",
            "Epoch [38/100], Loss: 599.3111, Train Accuracy: 73.22%\n",
            "Epoch [39/100], Loss: 595.9143, Train Accuracy: 73.39%\n",
            "Epoch [40/100], Loss: 597.6391, Train Accuracy: 73.22%\n",
            "Epoch [41/100], Loss: 591.6510, Train Accuracy: 73.64%\n",
            "Epoch [42/100], Loss: 588.9879, Train Accuracy: 73.56%\n",
            "Epoch [43/100], Loss: 588.1310, Train Accuracy: 73.65%\n",
            "Epoch [44/100], Loss: 584.1819, Train Accuracy: 73.81%\n",
            "Epoch [45/100], Loss: 581.3166, Train Accuracy: 74.04%\n",
            "Epoch [46/100], Loss: 577.5813, Train Accuracy: 74.22%\n",
            "Epoch [47/100], Loss: 575.5615, Train Accuracy: 74.19%\n",
            "Epoch [48/100], Loss: 573.0343, Train Accuracy: 74.54%\n",
            "Epoch [49/100], Loss: 569.4913, Train Accuracy: 74.68%\n",
            "Epoch [50/100], Loss: 565.0414, Train Accuracy: 74.91%\n",
            "Epoch [51/100], Loss: 564.8716, Train Accuracy: 74.90%\n",
            "Epoch [52/100], Loss: 560.5162, Train Accuracy: 75.16%\n",
            "Epoch [53/100], Loss: 560.4173, Train Accuracy: 75.13%\n",
            "Epoch [54/100], Loss: 558.4962, Train Accuracy: 75.13%\n",
            "Epoch [55/100], Loss: 553.6155, Train Accuracy: 75.38%\n",
            "Epoch [56/100], Loss: 552.1396, Train Accuracy: 75.28%\n",
            "Epoch [57/100], Loss: 547.6946, Train Accuracy: 75.55%\n",
            "Epoch [58/100], Loss: 546.2956, Train Accuracy: 75.42%\n",
            "Epoch [59/100], Loss: 545.3711, Train Accuracy: 75.56%\n",
            "Epoch [60/100], Loss: 540.7971, Train Accuracy: 75.66%\n",
            "Epoch [61/100], Loss: 538.7072, Train Accuracy: 75.80%\n",
            "Epoch [62/100], Loss: 543.1274, Train Accuracy: 75.71%\n",
            "Epoch [63/100], Loss: 539.8610, Train Accuracy: 75.87%\n",
            "Epoch [64/100], Loss: 539.2614, Train Accuracy: 76.10%\n",
            "Epoch [65/100], Loss: 534.1918, Train Accuracy: 76.09%\n",
            "Epoch [66/100], Loss: 532.4497, Train Accuracy: 76.22%\n",
            "Epoch [67/100], Loss: 528.5658, Train Accuracy: 76.28%\n",
            "Epoch [68/100], Loss: 527.6135, Train Accuracy: 76.38%\n",
            "Epoch [69/100], Loss: 522.9354, Train Accuracy: 76.55%\n",
            "Epoch [70/100], Loss: 527.4259, Train Accuracy: 76.51%\n",
            "Epoch [71/100], Loss: 522.0754, Train Accuracy: 76.71%\n",
            "Epoch [72/100], Loss: 522.3565, Train Accuracy: 76.60%\n",
            "Epoch [73/100], Loss: 521.8926, Train Accuracy: 76.69%\n",
            "Epoch [74/100], Loss: 518.7590, Train Accuracy: 76.61%\n",
            "Epoch [75/100], Loss: 515.6633, Train Accuracy: 76.83%\n",
            "Epoch [76/100], Loss: 518.3421, Train Accuracy: 76.86%\n",
            "Epoch [77/100], Loss: 511.6709, Train Accuracy: 77.15%\n",
            "Epoch [78/100], Loss: 513.3631, Train Accuracy: 76.95%\n",
            "Epoch [79/100], Loss: 508.4981, Train Accuracy: 77.17%\n"
          ]
        }
      ]
    }
  ]
}